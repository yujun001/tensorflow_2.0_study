{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-3,激活函数activation\n",
    "\n",
    "激活函数在深度学习中扮演着非常重要的角色，它给网络赋予了非线性，从而使得神经网络能够拟合任意复杂的函数。\n",
    "\n",
    "如果没有激活函数，无论多复杂的网络，都等价于单一的线性变换，无法对非线性函数进行拟合。\n",
    "\n",
    "目前，深度学习中最流行的激活函数为 relu, 但也有些新推出的激活函数，例如 swish、GELU 据称效果优于relu激活函数。\n",
    "\n",
    "激活函数的综述介绍可以参考下面两篇文章。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，常用激活函数\n",
    "\n",
    "\n",
    "* tf.nn.sigmoid：将实数压缩到0到1之间，一般只在二分类的最后输出层使用。主要缺陷为存在梯度消失问题，计算复杂度高，输出不以0为中心。\n",
    "\n",
    "* tf.nn.softmax：sigmoid的多分类扩展，一般只在多分类问题的最后输出层使用。\n",
    "\n",
    "* tf.nn.tanh：将实数压缩到-1到1之间，输出期望为0。主要缺陷为存在梯度消失问题，计算复杂度高。\n",
    "\n",
    "* tf.nn.relu：修正线性单元，最流行的激活函数。一般隐藏层使用。主要缺陷是：输出不以0为中心，输入小于0时存在梯度消失问题(死亡relu)。\n",
    "\n",
    "* tf.nn.leaky_relu：对修正线性单元的改进，解决了死亡relu问题。\n",
    "\n",
    "* tf.nn.elu：指数线性单元。对relu的改进，能够缓解死亡relu问题。\n",
    "\n",
    "* tf.nn.selu：扩展型指数线性单元。在权重用tf.keras.initializers.lecun_normal初始化前提下能够对神经网络进行自归一化。不可能出现梯度爆炸或者梯度消失问题。需要和Dropout的变种AlphaDropout一起使用。\n",
    "\n",
    "* tf.nn.swish：自门控激活函数。谷歌出品，相关研究指出用swish替代relu将获得轻微效果提升。\n",
    "\n",
    "* gelu：高斯误差线性单元激活函数。在Transformer中表现最好。tf.nn模块尚没有实现该函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，在模型中使用激活函数\n",
    "在keras模型中使用激活函数一般有两种方式，\n",
    "一种是作为某些层的activation参数指定，\n",
    "另一种是显式添加layers.Activation激活层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T01:07:03.165111Z",
     "start_time": "2020-05-17T01:07:03.066443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, None, 32)          544       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 10)          330       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, None, 10)          0         \n",
      "=================================================================\n",
      "Total params: 874\n",
      "Trainable params: 874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32,input_shape = (None,16),activation = tf.nn.relu)) #通过activation参数指定\n",
    "model.add(layers.Dense(10))\n",
    "model.add(layers.Activation(tf.nn.softmax))  # 显式添加layers.Activation激活层\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-4,模型层layers\n",
    "深度学习模型一般由各种模型层组合而成。\n",
    "\n",
    "tf.keras.layers内置了非常丰富的各种功能的模型层。例如，\n",
    "\n",
    "layers.Dense,layers.Flatten,layers.Input,layers.DenseFeature,layers.Dropout\n",
    "\n",
    "layers.Conv2D,layers.MaxPooling2D,layers.Conv1D\n",
    "\n",
    "layers.Embedding,layers.GRU,layers.LSTM,layers.Bidirectional等等。\n",
    "\n",
    "如果这些内置模型层不能够满足需求，我们也可以通过编写tf.keras.Lambda匿名模型层或继承tf.keras.layers.Layer基类构建自定义的模型层。\n",
    "\n",
    "其中tf.keras.Lambda匿名模型层只适用于构造没有学习参数的模型层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，内置模型层\n",
    "一些常用的内置模型层简单介绍如下。\n",
    "\n",
    "**基础层**\n",
    "\n",
    "* Dense：密集连接层。参数个数 = 输入层特征数× 输出层特征数(weight)＋ 输出层特征数(bias)\n",
    "\n",
    "* Activation：激活函数层。一般放在Dense层后面，等价于在Dense层中指定activation。\n",
    "\n",
    "* Dropout：随机置零层。训练期间以一定几率将输入置0，一种正则化手段。\n",
    "\n",
    "* BatchNormalization：批标准化层。通过线性变换将输入批次缩放平移到稳定的均值和标准差。可以增强模型对输入不同分布的适应性，加快模型训练速度，有轻微正则化效果。一般在激活函数之前使用。\n",
    "\n",
    "* SpatialDropout2D：空间随机置零层。训练期间以一定几率将整个特征图置0，一种正则化手段，有利于避免特征图之间过高的相关性。\n",
    "\n",
    "* Input：输入层。通常使用Functional API方式构建模型时作为第一层。\n",
    "\n",
    "* DenseFeature：特征列接入层，用于接收一个特征列列表并产生一个密集连接层。\n",
    "\n",
    "* Flatten：压平层，用于将多维张量压成一维。\n",
    "\n",
    "* Reshape：形状重塑层，改变输入张量的形状。\n",
    "\n",
    "* Concatenate：拼接层，将多个张量在某个维度上拼接。\n",
    "\n",
    "* Add：加法层。\n",
    "\n",
    "* Subtract： 减法层。\n",
    "\n",
    "* Maximum：取最大值层。\n",
    "\n",
    "* Minimum：取最小值层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**卷积网络相关层**\n",
    "\n",
    "* Conv1D：普通一维卷积，常用于文本。参数个数 = 输入通道数×卷积核尺寸(如3)×卷积核个数\n",
    "\n",
    "* Conv2D：普通二维卷积，常用于图像。参数个数 = 输入通道数×卷积核尺寸(如3乘3)×卷积核个数\n",
    "\n",
    "* Conv3D：普通三维卷积，常用于视频。参数个数 = 输入通道数×卷积核尺寸(如3乘3乘3)×卷积核个数\n",
    "\n",
    "* SeparableConv2D：二维深度可分离卷积层。不同于普通卷积同时对区域和通道操作，深度可分离卷积先操作区域，再操作通道。即先对每个通道做独立卷积操作区域，再用1乘1卷积跨通道组合操作通道。参数个数 = 输入通道数×卷积核尺寸 + 输入通道数×1×1×输出通道数。深度可分离卷积的参数数量一般远小于普通卷积，效果一般也更好。\n",
    "\n",
    "* DepthwiseConv2D：二维深度卷积层。仅有SeparableConv2D前半部分操作，即只操作区域，不操作通道，一般输出通道数和输入通道数相同，但也可以通过设置depth_multiplier让输出通道为输入通道的若干倍数。输出通道数 = 输入通道数 × depth_multiplier。参数个数 = 输入通道数×卷积核尺寸× depth_multiplier。\n",
    "\n",
    "* Conv2DTranspose：二维卷积转置层，俗称反卷积层。并非卷积的逆操作，但在卷积核相同的情况下，当其输入尺寸是卷积操作输出尺寸的情况下，卷积转置的输出尺寸恰好是卷积操作的输入尺寸。\n",
    "\n",
    "* LocallyConnected2D: 二维局部连接层。类似Conv2D，唯一的差别是没有空间上的权值共享，所以其参数个数远高于二维卷积。\n",
    "\n",
    "* MaxPool2D: 二维最大池化层。也称作下采样层。池化层无可训练参数，主要作用是降维。\n",
    "\n",
    "* AveragePooling2D: 二维平均池化层。\n",
    "\n",
    "* GlobalMaxPool2D: 全局最大池化层。每个通道仅保留一个值。一般从卷积层过渡到全连接层时使用，是Flatten的替代方案。\n",
    "\n",
    "* GlobalAvgPool2D: 全局平均池化层。每个通道仅保留一个值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**循环网络相关层**\n",
    "\n",
    "* Embedding：嵌入层。一种比Onehot更加有效的对离散特征进行编码的方法。一般用于将输入中的单词映射为稠密向量。嵌入层的参数需要学习。\n",
    "\n",
    "* LSTM：长短记忆循环网络层。最普遍使用的循环网络层。具有携带轨道，遗忘门，更新门，输出门。可以较为有效地缓解梯度消失问题，从而能够适用长期依赖问题。设置return_sequences = True时可以返回各个中间步骤输出，否则只返回最终输出。\n",
    "\n",
    "* GRU：门控循环网络层。LSTM的低配版，不具有携带轨道，参数数量少于LSTM，训练速度更快。\n",
    "\n",
    "* SimpleRNN：简单循环网络层。容易存在梯度消失，不能够适用长期依赖问题。一般较少使用。\n",
    "\n",
    "* ConvLSTM2D：卷积长短记忆循环网络层。结构上类似LSTM，但对输入的转换操作和对状态的转换操作都是卷积运算。\n",
    "\n",
    "* Bidirectional：双向循环网络包装器。可以将LSTM，GRU等层包装成双向循环网络。从而增强特征提取能力。\n",
    "\n",
    "* RNN：RNN基本层。接受一个循环网络单元或一个循环单元列表，通过调用tf.keras.backend.rnn函数在序列上进行迭代从而转换成循环网络层。\n",
    "\n",
    "* LSTMCell：LSTM单元。和LSTM在整个序列上迭代相比，它仅在序列上迭代一步。可以简单理解LSTM即RNN基本层包裹LSTMCell。\n",
    "\n",
    "* GRUCell：GRU单元。和GRU在整个序列上迭代相比，它仅在序列上迭代一步。\n",
    "\n",
    "* SimpleRNNCell：SimpleRNN单元。和SimpleRNN在整个序列上迭代相比，它仅在序列上迭代一步。\n",
    "\n",
    "* AbstractRNNCell：抽象RNN单元。通过对它的子类化用户可以自定义RNN单元，再通过RNN基本层的包裹实现用户自定义循环网络层。\n",
    "\n",
    "* Attention：Dot-product类型注意力机制层。可以用于构建注意力模型。\n",
    "\n",
    "* AdditiveAttention：Additive类型注意力机制层。可以用于构建注意力模型。\n",
    "\n",
    "* TimeDistributed：时间分布包装器。包装后可以将Dense、Conv2D等作用到每一个时间片段上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，自定义模型层\n",
    "\n",
    "如果自定义模型层没有需要被训练的参数，一般推荐使用Lamda层实现。\n",
    "\n",
    "如果自定义模型层有需要被训练的参数，则可以通过对Layer基类子类化实现。\n",
    "\n",
    "Lambda层由于没有需要被训练的参数，只需要定义正向传播逻辑即可，使用比Layer基类子类化更加简单。\n",
    "\n",
    "Lambda层的正向逻辑可以使用Python的lambda函数来表达，也可以用def关键字定义函数来表达。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T01:35:54.294969Z",
     "start_time": "2020-05-17T01:35:54.259124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=233, shape=(5,), dtype=int32, numpy=array([ 0,  1,  4,  9, 16], dtype=int32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models,regularizers\n",
    "\n",
    "mypower = layers.Lambda(lambda x:tf.math.pow(x,2))\n",
    "mypower(tf.range(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer的子类化一般需要重新实现初始化方法，Build方法和Call方法。下面是一个简化的线性层的范例，类似Dense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T01:36:24.228789Z",
     "start_time": "2020-05-17T01:36:24.179899Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(layers.Layer):\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "    \n",
    "    #build方法一般定义Layer需要被训练的参数。    \n",
    "    def build(self, input_shape): \n",
    "        self.w = self.add_weight(\"w\",shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True) #注意必须要有参数名称\"w\",否则会报错\n",
    "        self.b = self.add_weight(\"b\",shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        super(Linear,self).build(input_shape) # 相当于设置self.built = True\n",
    "\n",
    "    #call方法一般定义正向传播运算逻辑，__call__方法调用了它。  \n",
    "    @tf.function\n",
    "    def call(self, inputs): \n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "    #如果要让自定义的Layer通过Functional API 组合成模型时可以被保存成h5模型，需要自定义get_config方法。\n",
    "    def get_config(self):  \n",
    "        config = super(Linear, self).get_config()\n",
    "        config.update({'units': self.units})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T01:36:35.837276Z",
     "start_time": "2020-05-17T01:36:35.817503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "linear = Linear(units = 8)\n",
    "print(linear.built)\n",
    "#指定input_shape，显式调用build方法，第0维代表样本数量，用None填充\n",
    "linear.build(input_shape = (None,16)) \n",
    "print(linear.built)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T01:36:47.677463Z",
     "start_time": "2020-05-17T01:36:47.229548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "(None, 8)\n"
     ]
    }
   ],
   "source": [
    "linear = Linear(units = 8)\n",
    "print(linear.built)\n",
    "linear.build(input_shape = (None,16)) \n",
    "print(linear.compute_output_shape(input_shape = (None,16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T01:36:57.815236Z",
     "start_time": "2020-05-17T01:36:57.766162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "{'name': 'linear_2', 'trainable': True, 'dtype': 'float32', 'units': 16}\n"
     ]
    }
   ],
   "source": [
    "linear = Linear(units = 16)\n",
    "print(linear.built)\n",
    "#如果built = False，调用__call__时会先调用build方法, 再调用call方法。\n",
    "linear(tf.random.uniform((100,64))) \n",
    "print(linear.built)\n",
    "config = linear.get_config()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T01:37:11.362139Z",
     "start_time": "2020-05-17T01:37:11.316929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.input_shape:  (None, 2)\n",
      "model.output_shape:  (None, 1)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "linear (Linear)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "#注意该处的input_shape会被模型加工，无需使用None代表样本数量维\n",
    "model.add(Linear(units = 1,input_shape = (2,)))  \n",
    "print(\"model.input_shape: \",model.input_shape)\n",
    "print(\"model.output_shape: \",model.output_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T01:37:28.684502Z",
     "start_time": "2020-05-17T01:37:27.122040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04950666]\n",
      " [0.01868409]]\n",
      "[[0.04950666]\n",
      " [0.01868409]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0517 09:37:28.272590 140736614536128 deprecation.py:506] From /anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04950666]\n",
      " [0.01868409]]\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = \"sgd\",loss = \"mse\",metrics=[\"mae\"])\n",
    "print(model.predict(tf.constant([[3.0,2.0],[4.0,5.0]])))\n",
    "\n",
    "\n",
    "# 保存成 h5模型\n",
    "model.save(\"./data/linear_model.h5\",save_format = \"h5\")\n",
    "model_loaded_keras = tf.keras.models.load_model(\n",
    "    \"./data/linear_model.h5\",custom_objects={\"Linear\":Linear})\n",
    "print(model_loaded_keras.predict(tf.constant([[3.0,2.0],[4.0,5.0]])))\n",
    "\n",
    "\n",
    "# 保存成 tf模型\n",
    "model.save(\"./data/linear_model\",save_format = \"tf\")\n",
    "model_loaded_tf = tf.keras.models.load_model(\"./data/linear_model\")\n",
    "print(model_loaded_tf.predict(tf.constant([[3.0,2.0],[4.0,5.0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-5,损失函数losses\n",
    "\n",
    "一般来说，监督学习的目标函数由损失函数和正则化项组成。（Objective = Loss + Regularization）\n",
    "\n",
    "对于keras模型，目标函数中的正则化项一般在各层中指定，\n",
    "\n",
    "    例如使用Dense的 kernel_regularizer 和 bias_regularizer等参数指定权重使用l1或者l2正则化项，\n",
    "    此外还可以用kernel_constraint 和 bias_constraint等参数约束权重的取值范围，这也是一种正则化手段。\n",
    "\n",
    "损失函数在模型编译时候指定。\n",
    "\n",
    "对于回归模型，通常使用的损失函数是均方损失函数 mean_squared_error。\n",
    "\n",
    "对于二分类模型，通常使用的是二元交叉熵损失函数 binary_crossentropy。\n",
    "\n",
    "对于多分类模型，\n",
    "    \n",
    "    如果label是one-hot编码的，则使用类别交叉熵损失函数 categorical_crossentropy。\n",
    "    如果label是类别序号编码的，则需要使用稀疏类别交叉熵损失函数 sparse_categorical_crossentropy。\n",
    "\n",
    "如果有需要，也可以自定义损失函数，自定义损失函数需要接收两个张量y_true,y_pred作为输入参数，并输出一个标量作为损失函数值。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T01:51:55.020759Z",
     "start_time": "2020-05-17T01:51:55.013225Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models,losses,regularizers,constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T01:54:44.654152Z",
     "start_time": "2020-05-17T01:54:44.320835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 4,810\n",
      "Trainable params: 4,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### 一，损失函数和正则化项\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, \n",
    "                       input_dim=64,\n",
    "                       kernel_regularizer=regularizers.l2(0.01), \n",
    "                       activity_regularizer=regularizers.l1(0.01),\n",
    "                       kernel_constraint = constraints.MaxNorm(max_value=2, axis=0))) \n",
    "model.add(layers.Dense(10,\n",
    "                       kernel_regularizer=regularizers.l1_l2(0.01,0.01),\n",
    "                       activation = \"sigmoid\"))\n",
    "model.compile(optimizer = \"rmsprop\",\n",
    "              loss = \"sparse_categorical_crossentropy\",\n",
    "              metrics = [\"AUC\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T01:55:28.948093Z",
     "start_time": "2020-05-17T01:55:28.918873Z"
    }
   },
   "source": [
    "### 二，内置损失函数\n",
    "\n",
    "内置的损失函数一般有类的实现和函数的实现两种形式。\n",
    "\n",
    "如：CategoricalCrossentropy 和 categorical_crossentropy 都是类别交叉熵损失函数，前者是类的实现形式，后者是函数的实现形式。\n",
    "\n",
    "常用的一些内置损失函数说明如下。\n",
    "\n",
    "* mean_squared_error（均方误差损失，用于回归，简写为 mse, 类与函数实现形式分别为 MeanSquaredError 和 MSE）\n",
    "\n",
    "* mean_absolute_error (平均绝对值误差损失，用于回归，简写为 mae, 类与函数实现形式分别为 MeanAbsoluteError 和 MAE)\n",
    "\n",
    "* mean_absolute_percentage_error (平均百分比误差损失，用于回归，简写为 mape, 类与函数实现形式分别为 MeanAbsolutePercentageError 和 MAPE)\n",
    "\n",
    "* Huber(Huber损失，只有类实现形式，用于回归，介于mse和mae之间，对异常值比较鲁棒，相对mse有一定的优势)\n",
    "\n",
    "\n",
    "\n",
    "* binary_crossentropy(二元交叉熵，用于二分类，类实现形式为 BinaryCrossentropy)\n",
    "\n",
    "* categorical_crossentropy(类别交叉熵，用于多分类，要求label为onehot编码，类实现形式为 CategoricalCrossentropy)\n",
    "\n",
    "* sparse_categorical_crossentropy(稀疏类别交叉熵，用于多分类，要求label为序号编码形式，类实现形式为 SparseCategoricalCrossentropy)\n",
    "\n",
    "\n",
    "\n",
    "* hinge(合页损失函数，用于二分类，最著名的应用是作为支持向量机SVM的损失函数，类实现形式为 Hinge)\n",
    "\n",
    "* kld(相对熵损失，也叫KL散度，常用于最大期望算法EM的损失函数，两个概率分布差异的一种信息度量。类与函数实现形式分别为 KLDivergence 或 KLD)\n",
    "\n",
    "* cosine_similarity(余弦相似度，可用于多分类，类实现形式为 CosineSimilarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三，自定义损失函数\n",
    "\n",
    "\n",
    "自定义损失函数接收两个张量y_true,y_pred作为输入参数，并输出一个标量作为损失函数值。\n",
    "\n",
    "也可以对tf.keras.losses.Loss进行子类化，重写call方法实现损失的计算逻辑，从而得到损失函数的类的实现。\n",
    "\n",
    "下面是一个Focal Loss的自定义实现示范。Focal Loss是一种对binary_crossentropy的改进损失函数形式。\n",
    "\n",
    "在类别不平衡和存在难以训练样本的情形下相对于二元交叉熵能够取得更好的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T01:59:22.460388Z",
     "start_time": "2020-05-17T01:59:22.407475Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2., alpha=0.25):\n",
    "    \n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        loss = -tf.reduce_sum(alpha * tf.pow(1. - pt_1, gamma) * tf.log(1e-07+pt_1)) \\\n",
    "           -tf.reduce_sum((1-alpha) * tf.pow( pt_0, gamma) * tf.log(1. - pt_0 + 1e-07))\n",
    "        return loss\n",
    "    return focal_loss_fixed\n",
    "\n",
    "\n",
    "class FocalLoss(losses.Loss):\n",
    "    \n",
    "    def __init__(self,gamma=2.0,alpha=0.25):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def call(self,y_true,y_pred):\n",
    "        \n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        loss = -tf.reduce_sum(self.alpha * tf.pow(1. - pt_1, self.gamma) * tf.log(1e-07+pt_1)) \\\n",
    "           -tf.reduce_sum((1-self.alpha) * tf.pow( pt_0, self.gamma) * tf.log(1. - pt_0 + 1e-07))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-6,评估指标metrics\n",
    "\n",
    "损失函数除了作为模型训练时候的优化目标，也能够作为模型好坏的一种评价指标。但通常人们还会从其它角度评估模型的好坏。\n",
    "\n",
    "这就是评估指标。通常损失函数都可以作为评估指标，如MAE,MSE,CategoricalCrossentropy等也是常用的评估指标。\n",
    "\n",
    "但评估指标不一定可以作为损失函数，例如AUC,Accuracy,Precision。因为评估指标不要求连续可导，而损失函数通常要求连续可导。\n",
    "\n",
    "编译模型时，可以通过列表形式指定多个评估指标。\n",
    "\n",
    "如果有需要，也可以自定义评估指标。\n",
    "\n",
    "自定义评估指标需要接收两个张量y_true,y_pred作为输入参数，并输出一个标量作为评估值。\n",
    "\n",
    "也可以对tf.keras.metrics.Metric进行子类化，重写初始化方法, update_state方法, result方法实现评估指标的计算逻辑，从而得到评估指标的类的实现形式。\n",
    "\n",
    "由于训练的过程通常是分批次训练的，而评估指标要跑完一个epoch才能够得到整体的指标结果。因此，类形式的评估指标更为常见。即需要编写初始化方法以创建与计算指标结果相关的一些中间变量，编写update_state方法在每个batch后更新相关中间变量的状态，编写result方法输出最终指标结果。\n",
    "\n",
    "如果编写函数形式的评估指标，则只能取epoch中各个batch计算的评估指标结果的平均值作为整个epoch上的评估指标结果，这个结果通常会偏离整个epoch数据一次计算的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，常用的内置评估指标\n",
    "\n",
    "\n",
    "* MeanSquaredError（均方误差，用于回归，可以简写为MSE，函数形式为mse）\n",
    "\n",
    "* MeanAbsoluteError (平均绝对值误差，用于回归，可以简写为MAE，函数形式为mae)\n",
    "\n",
    "* MeanAbsolutePercentageError (平均百分比误差，用于回归，可以简写为MAPE，函数形式为mape)\n",
    "\n",
    "* RootMeanSquaredError (均方根误差，用于回归)\n",
    "\n",
    "\n",
    "\n",
    "* Accuracy (准确率，用于分类，可以用字符串\"Accuracy\"表示，Accuracy=(TP+TN)/(TP+TN+FP+FN)，要求y_true和y_pred都为类别序号编码)\n",
    "\n",
    "* Precision (精确率，用于二分类，Precision = TP/(TP+FP))\n",
    "\n",
    "* Recall (召回率，用于二分类，Recall = TP/(TP+FN))\n",
    "\n",
    "* TruePositives (真正例，用于二分类)\n",
    "\n",
    "* TrueNegatives (真负例，用于二分类)\n",
    "\n",
    "* FalsePositives (假正例，用于二分类)\n",
    "\n",
    "* FalseNegatives (假负例，用于二分类)\n",
    "\n",
    "* AUC(ROC曲线(TPR vs FPR)下的面积，用于二分类，直观解释为随机抽取一个正样本和一个负样本，正样本的预测值大于负样本的概率)\n",
    "\n",
    "* CategoricalAccuracy（分类准确率，与Accuracy含义相同，要求y_true(label)为onehot编码形式）\n",
    "\n",
    "* SparseCategoricalAccuracy (稀疏分类准确率，与Accuracy含义相同，要求y_true(label)为序号编码形式)\n",
    "\n",
    "* MeanIoU (Intersection-Over-Union，常用于图像分割)\n",
    "\n",
    "* TopKCategoricalAccuracy (多分类TopK准确率，要求y_true(label)为onehot编码形式)\n",
    "\n",
    "* SparseTopKCategoricalAccuracy (稀疏多分类TopK准确率，要求y_true(label)为序号编码形式)\n",
    "\n",
    "* Mean (平均值)\n",
    "\n",
    "* Sum (求和)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二， 自定义评估指标\n",
    "\n",
    "\n",
    "我们以金融风控领域常用的KS指标为例，示范自定义评估指标。\n",
    "\n",
    "KS指标适合二分类问题，其计算方式为 KS=max(TPR-FPR).\n",
    "\n",
    "其中TPR=TP/(TP+FN) , FPR = FP/(FP+TN) \n",
    "\n",
    "TPR曲线实际上就是正样本的累积分布曲线(CDF)，FPR曲线实际上就是负样本的累积分布曲线(CDF)。\n",
    "\n",
    "KS指标就是正样本和负样本累积分布曲线差值的最大值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:06:26.098291Z",
     "start_time": "2020-05-17T02:06:26.069962Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models,losses,metrics\n",
    "\n",
    "#函数形式的自定义评估指标\n",
    "@tf.function\n",
    "def ks(y_true,y_pred):\n",
    "    y_true = tf.reshape(y_true,(-1,))\n",
    "    y_pred = tf.reshape(y_pred,(-1,))\n",
    "    length = tf.shape(y_true)[0]\n",
    "    t = tf.math.top_k(y_pred,k = length,sorted = False)\n",
    "    y_pred_sorted = tf.gather(y_pred,t.indices)\n",
    "    y_true_sorted = tf.gather(y_true,t.indices)\n",
    "    cum_positive_ratio = tf.truediv(\n",
    "        tf.cumsum(y_true_sorted),tf.reduce_sum(y_true_sorted))\n",
    "    cum_negative_ratio = tf.truediv(\n",
    "        tf.cumsum(1 - y_true_sorted),tf.reduce_sum(1 - y_true_sorted))\n",
    "    ks_value = tf.reduce_max(tf.abs(cum_positive_ratio - cum_negative_ratio)) \n",
    "    return ks_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:07:09.528827Z",
     "start_time": "2020-05-17T02:07:08.994807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.constant([[1],[1],[1],[0],[1],[1],[1],[0],[0],[0],[1],[0],[1],[0]])\n",
    "y_pred = tf.constant([[0.6],[0.1],[0.4],[0.5],[0.7],[0.7],[0.7],\n",
    "                      [0.4],[0.4],[0.5],[0.8],[0.3],[0.5],[0.3]])\n",
    "tf.print(ks(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:07:48.577970Z",
     "start_time": "2020-05-17T02:07:48.315777Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#类形式的自定义评估指标\n",
    "class KS(metrics.Metric):\n",
    "    \n",
    "    def __init__(self, name = \"ks\", **kwargs):\n",
    "        super(KS,self).__init__(name=name,**kwargs)\n",
    "        self.true_positives = self.add_weight(\n",
    "            name = \"tp\",shape = (101,), initializer = \"zeros\")\n",
    "        self.false_positives = self.add_weight(\n",
    "            name = \"fp\",shape = (101,), initializer = \"zeros\")\n",
    "   \n",
    "    @tf.function\n",
    "    def update_state(self,y_true,y_pred):\n",
    "        y_true = tf.cast(tf.reshape(y_true,(-1,)),tf.bool)\n",
    "        y_pred = tf.cast(100*tf.reshape(y_pred,(-1,)),tf.int32)\n",
    "        \n",
    "        for i in tf.range(0,tf.shape(y_true)[0]):\n",
    "            if y_true[i]:\n",
    "                self.true_positives[y_pred[i]].assign(\n",
    "                    self.true_positives[y_pred[i]]+1.0)\n",
    "            else:\n",
    "                self.false_positives[y_pred[i]].assign(\n",
    "                    self.false_positives[y_pred[i]]+1.0)\n",
    "        return (self.true_positives,self.false_positives)\n",
    "    \n",
    "    @tf.function\n",
    "    def result(self):\n",
    "        cum_positive_ratio = tf.truediv(\n",
    "            tf.cumsum(self.true_positives),tf.reduce_sum(self.true_positives))\n",
    "        cum_negative_ratio = tf.truediv(\n",
    "            tf.cumsum(self.false_positives),tf.reduce_sum(self.false_positives))\n",
    "        ks_value = tf.reduce_max(tf.abs(cum_positive_ratio - cum_negative_ratio)) \n",
    "        return ks_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:08:05.310854Z",
     "start_time": "2020-05-17T02:08:04.232972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.constant([[1],[1],[1],[0],[1],[1],[1],[0],[0],[0],[1],[0],[1],[0]])\n",
    "y_pred = tf.constant([[0.6],[0.1],[0.4],[0.5],[0.7],[0.7],\n",
    "                      [0.7],[0.4],[0.4],[0.5],[0.8],[0.3],[0.5],[0.3]])\n",
    "\n",
    "myks = KS()\n",
    "myks.update_state(y_true,y_pred)\n",
    "tf.print(myks.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-7,优化器optimizers\n",
    "\n",
    "深度学习优化算法大概经历了 SGD -> SGDM -> NAG ->Adagrad -> Adadelta(RMSprop) -> Adam -> Nadam 这样的发展历程。\n",
    "\n",
    "对于一般新手炼丹师，优化器直接使用Adam，并使用其默认参数就OK了。\n",
    "\n",
    "一些爱写论文的炼丹师由于追求评估指标效果，可能会偏爱前期使用Adam优化器快速下降，后期使用SGD并精调优化器参数得到更好的结果。\n",
    "\n",
    "此外目前也有一些前沿的优化算法，据称效果比Adam更好，例如LazyAdam, Look-ahead, RAdam, Ranger等."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一，优化器的使用\n",
    "\n",
    "优化器主要使用apply_gradients方法传入变量和对应梯度从而来对给定变量进行迭代，或者直接使用minimize方法对目标函数进行迭代优化。\n",
    "\n",
    "当然，更常见的使用是在编译时将优化器传入keras的Model,通过调用model.fit实现对Loss的的迭代优化。\n",
    "\n",
    "初始化优化器时会创建一个变量optimier.iterations用于记录迭代的次数。因此优化器和tf.Variable一样，一般需要在@tf.function外创建。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:12:41.170359Z",
     "start_time": "2020-05-17T02:12:41.139801Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "#打印时间分割线\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:13:58.487635Z",
     "start_time": "2020-05-17T02:13:56.513970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================10:13:58\n",
      "step =  100\n",
      "x =  0.867380381\n",
      "\n",
      "================================================================================10:13:58\n",
      "step =  200\n",
      "x =  0.98241204\n",
      "\n",
      "================================================================================10:13:58\n",
      "step =  300\n",
      "x =  0.997667611\n",
      "\n",
      "================================================================================10:13:58\n",
      "step =  400\n",
      "x =  0.999690711\n",
      "\n",
      "================================================================================10:13:58\n",
      "step =  500\n",
      "x =  0.999959\n",
      "\n",
      "================================================================================10:13:58\n",
      "step =  600\n",
      "x =  0.999994516\n",
      "\n",
      "y = 0\n",
      "x = 0.999995232\n"
     ]
    }
   ],
   "source": [
    "# 求f(x) = a*x**2 + b*x + c的最小值\n",
    "\n",
    "# 使用optimizer.apply_gradients\n",
    "\n",
    "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "@tf.function\n",
    "def minimizef():\n",
    "    a = tf.constant(1.0)\n",
    "    b = tf.constant(-2.0)\n",
    "    c = tf.constant(1.0)\n",
    "    \n",
    "    while tf.constant(True): \n",
    "        with tf.GradientTape() as tape:\n",
    "            y = a*tf.pow(x,2) + b*x + c\n",
    "        dy_dx = tape.gradient(y,x)\n",
    "        optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])\n",
    "        \n",
    "        #迭代终止条件\n",
    "        if tf.abs(dy_dx)<tf.constant(0.00001):\n",
    "            break\n",
    "            \n",
    "        if tf.math.mod(optimizer.iterations,100)==0:\n",
    "            printbar()\n",
    "            tf.print(\"step = \",optimizer.iterations)\n",
    "            tf.print(\"x = \", x)\n",
    "            tf.print(\"\")\n",
    "                \n",
    "    y = a*tf.pow(x,2) + b*x + c\n",
    "    return y\n",
    "\n",
    "tf.print(\"y =\",minimizef())\n",
    "tf.print(\"x =\",x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:15:29.698277Z",
     "start_time": "2020-05-17T02:15:28.886944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  1000\n",
      "y =  0\n",
      "x =  0.999998569\n"
     ]
    }
   ],
   "source": [
    "# 求f(x) = a*x**2 + b*x + c的最小值\n",
    "\n",
    "# 使用optimizer.minimize\n",
    "\n",
    "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)   \n",
    "\n",
    "def f():   \n",
    "    a = tf.constant(1.0)\n",
    "    b = tf.constant(-2.0)\n",
    "    c = tf.constant(1.0)\n",
    "    y = a*tf.pow(x,2)+b*x+c\n",
    "    return(y)\n",
    "\n",
    "@tf.function\n",
    "def train(epoch = 1000):  \n",
    "    for _ in tf.range(epoch):  \n",
    "        optimizer.minimize(f,[x])\n",
    "    tf.print(\"epoch = \",optimizer.iterations)\n",
    "    return(f())\n",
    "\n",
    "train(1000)\n",
    "tf.print(\"y = \",f())\n",
    "tf.print(\"x = \",x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:16:56.502350Z",
     "start_time": "2020-05-17T02:16:53.954284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fake_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Total params: 1\n",
      "Trainable params: 1\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 100 samples\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 0s 4ms/sample - loss: 0.2481\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 0.0044\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 7.6740e-05\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 1.3500e-06\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 1.8477e-08\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 0.0000e+00 0s - loss: 0.0000e+0\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# 求f(x) = a*x**2 + b*x + c的最小值\n",
    "# 使用model.fit\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "class FakeModel(tf.keras.models.Model):\n",
    "    def __init__(self,a,b,c):\n",
    "        super(FakeModel,self).__init__()\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "    \n",
    "    def build(self):\n",
    "        self.x = tf.Variable(0.0,name = \"x\")\n",
    "        self.built = True\n",
    "    \n",
    "    def call(self,features):\n",
    "        loss  = self.a*(self.x)**2+self.b*(self.x)+self.c\n",
    "        return(tf.ones_like(features)*loss)\n",
    "    \n",
    "def myloss(y_true,y_pred):\n",
    "    return tf.reduce_mean(y_pred)\n",
    "\n",
    "model = FakeModel(tf.constant(1.0),tf.constant(-2.0),tf.constant(1.0))\n",
    "\n",
    "model.build()\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer = \n",
    "              tf.keras.optimizers.SGD(learning_rate=0.01),loss = myloss)\n",
    "history = model.fit(tf.zeros((100,2)),\n",
    "                    tf.ones(100),batch_size = 1,epochs = 10)  #迭代1000次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:17:02.588806Z",
     "start_time": "2020-05-17T02:17:02.575539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= 0.999998569\n",
      "loss= 0\n"
     ]
    }
   ],
   "source": [
    "tf.print(\"x=\",model.x)\n",
    "tf.print(\"loss=\",model(tf.constant(0.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二，内置优化器\n",
    "\n",
    "深度学习优化算法大概经历了 SGD -> SGDM -> NAG ->Adagrad -> Adadelta(RMSprop) -> Adam -> Nadam 这样的发展历程。\n",
    "\n",
    "在keras.optimizers子模块中，它们基本上都有对应的类的实现。\n",
    "\n",
    "* SGD, 默认参数为纯SGD, 设置momentum参数不为0实际上变成SGDM, 考虑了一阶动量, 设置 nesterov为True后变成NAG，即 Nesterov Accelerated Gradient，在计算梯度时计算的是向前走一步所在位置的梯度。\n",
    "\n",
    "* Adagrad, 考虑了二阶动量，对于不同的参数有不同的学习率，即自适应学习率。缺点是学习率单调下降，可能后期学习速率过慢乃至提前停止学习。\n",
    "\n",
    "* RMSprop, 考虑了二阶动量，对于不同的参数有不同的学习率，即自适应学习率，对Adagrad进行了优化，通过指数平滑只考虑一定窗口内的二阶动量。\n",
    "\n",
    "* Adadelta, 考虑了二阶动量，与RMSprop类似，但是更加复杂一些，自适应性更强。\n",
    "\n",
    "* Adam, 同时考虑了一阶动量和二阶动量，可以看成RMSprop上进一步考虑了一阶动量。\n",
    "\n",
    "* Nadam, 在Adam基础上进一步考虑了 Nesterov Acceleration。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
